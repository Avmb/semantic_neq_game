{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2192afc3-647d-4ce4-82dc-e8798a082315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, random\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "from datasets import load_dataset\n",
    "import tiktoken\n",
    "import mistune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2425cec0-1ed2-4a45-bcea-2d709f3b4265",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.6\n",
    "#model = \"gpt-4o-mini-2024-07-18\"\n",
    "model = \"ft:gpt-4o-mini-2024-07-18:uedin:semantic-inequiv-bob-run-0-round-6-generation-0:AyS2gJFd\"\n",
    "max_tokens = 2048\n",
    "max_input_tokens = 128000-max_tokens\n",
    "n = 1\n",
    "#n = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03d5fab7-fbe7-475a-b278-a82ced1f7913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename_base = \"./gpt-4o-mini\"\n",
    "filename_base = \"./sinq-gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d7a6882-873e-4a06-8d04-26d7a5834f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_to_openai = filename_base+\"_to_openai.jsonl\"\n",
    "filename_from_openai = filename_base+\"_from_openai.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1834d8b0-be52-4bf2-b189-2711a01ebea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"sunlab/PySecDB\", split=\"train\") # there is not \"test\"\n",
    "dataset = list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e025c49-95c5-4ff0-8623-e129fce4b54c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70074403-553a-4530-a83b-8e807170d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \\\n",
    "\"\"\"You are an expert computer scientist. Your task is to take a diff patch for a Python program and determine whether it contains a fix to a security vulnerability.\n",
    "Write your output the following JSON format:\n",
    "Think step by step before writing your program. Use the following Markdown format, making sure that the following sections are delimited by level 1 headings, since they will have to be automatically parsed:\n",
    "# Analysis\n",
    "step by step analysis. This section can include sub-headings and code blocks\n",
    "# Output\n",
    "```json\n",
    "{\n",
    "  \"vulnerability_fix\": true or false\n",
    "}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "def user_message_fn_0(example):\n",
    "    return f\"\"\"Diff patch:\n",
    "\n",
    "```\n",
    "{example[\"content\"]}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4e36820-2a1d-4fe7-840d-57a64530c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_to_openai_api_format(example):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}, \n",
    "                {\"role\": \"user\", \"content\": user_message_fn_0(example)}]\n",
    "    rv = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"n\": n,\n",
    "        #\"response_format\": { \"type\": \"json_object\" }\n",
    "    }\n",
    "    return json.dumps(rv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25df534b-57d8-4f2a-9bb9-73d62e3018ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename_to_openai, \"w\") as out_fs:\n",
    "    for example in dataset:\n",
    "        example[\"llm_request\"] = example_to_openai_api_format(example)\n",
    "        encoding = tiktoken.encoding_for_model(\"gpt-4o-mini-2024-07-18\")\n",
    "        example[\"llm_request_token_count\"] = len(encoding.encode(example[\"llm_request\"]))\n",
    "        if example[\"llm_request_token_count\"] > max_input_tokens:\n",
    "            del example[\"llm_request\"]\n",
    "        else:\n",
    "            print(example[\"llm_request\"], file=out_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22d59c44-8622-4f1b-82cd-794261b929b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b524112-48ca-48ff-9334-b828ff80bd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response\n"
     ]
    }
   ],
   "source": [
    "responses_from_openai = dict()\n",
    "with open(filename_from_openai, \"r\") as in_fs:\n",
    "    for line in in_fs:\n",
    "        response_k, response_v = json.loads(line)\n",
    "        response_k_str = json.dumps(response_k)\n",
    "        responses_from_openai[response_k_str] = response_v\n",
    "\n",
    "markdown = mistune.create_markdown(renderer='ast')\n",
    "for example in dataset:\n",
    "    if (\"llm_request\" in example) and (example[\"llm_request\"] in responses_from_openai):\n",
    "        example[\"llm_response\"] = responses_from_openai[example[\"llm_request\"]]\n",
    "        content = example[\"llm_response\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        try:\n",
    "            parsed_markdown = markdown(content)\n",
    "            output_i = [i for i, e in enumerate(parsed_markdown) if (e['type'] == \"heading\") and (e[\"children\"][0][\"raw\"] == \"Output\")][0]\n",
    "            output_code_block = [e[\"raw\"] for e in parsed_markdown[output_i:] if (e['type'] == \"block_code\") and (\"attrs\" in e) and (e[\"attrs\"][\"info\"]==\"json\")][0]\n",
    "            parsed = json.loads(output_code_block)\n",
    "            example[\"llm_prediction\"] = parsed[\"vulnerability_fix\"]\n",
    "        except:\n",
    "            print(\"Invalid response\")\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "934dc2a9-bee6-4fc4-8b70-1fe4bbf4369f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ft:gpt-4o-mini-2024-07-18:uedin:semantic-inequiv-bob-run-0-round-6-generation-0:AyS2gJFd, n: 1, temperature: 0.6, evaluated examples: 4041, num. correct: 2950, accuracy: 0.7300173224449393\n"
     ]
    }
   ],
   "source": [
    "num_examples = 0\n",
    "num_correct = 0\n",
    "\n",
    "for example in dataset:\n",
    "    if \"llm_request\" in example:\n",
    "        num_examples += 1\n",
    "        try:\n",
    "            majority_prediction = (np.array(example[\"llm_prediction\"]).mean() >= 0.5)\n",
    "            if (example[\"label\"] == \"security\") == majority_prediction:\n",
    "                num_correct += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "accuracy = float(num_correct) / num_examples\n",
    "print(f\"Model: {model}, n: {n}, temperature: {temperature}, evaluated examples: {num_examples}, num. correct: {num_correct}, accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58c85a35-3784-4018-8dc1-73a7e9dd4d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: gpt-4o-mini-2024-07-18, n: 1, temperature: 0.6, evaluated examples: 4041, num. correct: 2972, accuracy: 0.7354615194258847\n",
    "# Model: ft:gpt-4o-mini-2024-07-18:uedin:semantic-inequiv-bob-run-0-round-6-generation-0:AyS2gJFd, n: 1, temperature: 0.6, evaluated examples: 4041, num. correct: 2950, accuracy: 0.7300173224449393"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e49451-8843-41dc-a029-a12869ff5869",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
